|.if X64
|.arch x64
|.else
|.arch x86
|.endif

|.actionlist build_actionlist
|.globalnames globnames
|.externnames extnames

|.if not X64
|.define RET_H, edx // for int64_t returns
|.define RET_L, eax
|.endif

|.if X64WIN
|.macro call_piip, func, arg0, arg1, arg2, arg3
| mov r9, arg3
| mov r8d, arg2
| mov edx, arg1
| mov rcx, arg0
| call func
|.endmacro
|.macro call_pip, func, arg0, arg1, arg2
| mov r8, arg2
| mov edx, arg1
| mov rcx, arg0
| call func
|.endmacro
|.macro call_pp, func, arg0, arg1
| mov rdx, arg1
| mov rcx, arg0
| call func
|.endmacro
|.macro call_pi, func, arg0, arg1
| mov edx, arg1
| mov rcx, arg0
| call func
|.endmacro
|.macro call_i, func, arg0
| mov ecx, arg0
| call func
|.endmacro
|
|.elif X64
|.macro call_piiiip, func, arg0, arg1, arg2, arg3, arg4, arg5
| mov r9, arg5
| mov r8d, arg4
| mov ecx, arg3
| mov edx, arg2
| mov esi, arg1
| mov rdi, arg0
| call func
|.endmacro
|.macro call_piiip, func, arg0, arg1, arg2, arg3, arg4
| mov r8, arg4
| mov ecx, arg3
| mov edx, arg2
| mov esi, arg1
| mov rdi, arg0
| call func
|.endmacro
|.macro call_piip, func, arg0, arg1, arg2, arg3
| mov rcx, arg3
| mov edx, arg2
| mov esi, arg1
| mov rdi, arg0
| call func
|.endmacro
|.macro call_pip, func, arg0, arg1, arg2
| mov rdx, arg2
| mov esi, arg1
| mov rdi, arg0
| call func
|.endmacro
|.macro call_pp, func, arg0, arg1
| mov rsi, arg1
| mov rdi, arg0
| call func
|.endmacro
|.macro call_pi, func, arg0, arg1
| mov esi, arg1
| mov rdi, arg0
| call func
|.endmacro
|.macro call_i, func, arg0
| mov edi, arg0
| call func
|.endmacro
|
|.else
| // define the 64bit registers to the 32 bit counterparts, so the common
| // code can use r*x for all pointers
|.define rax, eax
|.define rcx, ecx
|.define rdx, edx
|.define rsp, esp
|.define rbp, ebp
|.define rdi, edi
|.define rsi, esi
|.macro call_piip, func, arg0, arg1, arg2, arg3
| mov dword [rsp+12], arg3
| mov dword [rsp+8], arg2
| mov dword [rsp+4], arg1
| mov dword [rsp], arg0
| call func
|.endmacro
|.macro call_pip, func, arg0, arg1, arg2
| mov dword [rsp+8], arg2
| mov dword [rsp+4], arg1
| mov dword [rsp], arg0
| call func
|.endmacro
|.macro call_pp, func, arg0, arg1
| mov dword [rsp+4], arg1
| mov dword [rsp], arg0
| call func
|.endmacro
|.macro call_pi, func, arg0, arg1
| call_pp, func, arg0, arg1
|.endmacro
|.macro call_i, func, arg0
| mov dword [rsp], arg0
| call func
|.endmacro
|
|.endif

#if defined _WIN64 || defined __amd64__
#define JUMP_SIZE (16)
#else
#define JUMP_SIZE (8)
#endif

#ifdef NDEBUG
#define shred(a,b,c)
#else
#define shred(p,s,e) memset((p)+(s),0xCC,(e)-(s))
#endif

static void compile_errno_load(jit_t* jit, lua_State* L, uint8_t* code)
{
    /* create a stub which returns the value of &jit->last_errno in rcx
     *
     * manually setup the commands so a) we can use rip relative displacement
     * and (dynasm doesn't support this) b) the jit state already has commands
     * buffered up
     */
#if defined _WIN64 || defined __amd64__
    /* lea rcx, [rip+1]
     * ret
     * &jit->last_errno
     */
    code[0] = 0x48; /* REX.W */
    code[1] = 0x8B; /* mov */
    code[2] = 0x0D; /* disp32 to rcx */
    *(int32_t*) &code[3] = 1;
    code[7] = 0xC3; /* ret */
    *(void**) &code[8] = &jit->last_errno;
    shred(code, 16, JUMP_SIZE);
#else
    /* lea rcx, &jit->last_errno
     * ret
     */
    code[0] = 0x8D; /* lea */
    code[1] = 0x0D; /* disp32 to ecx */
    *(void**) &code[2] = &jit->last_errno;
    code[6] = 0xC3; /* ret */
    shred(code, 7, JUMP_SIZE);
#endif
}

static void compile_extern_jump(jit_t* jit, lua_State* L, function_t func, uint8_t* code)
{
    /* The jump code is the function pointer followed by a stub to call the
     * function pointer. The stub exists in 64 bit so we can jump to functions
     * with an offset greater than 2 GB.
     *
     * Note we have to manually set this up since there are commands buffered
     * in the jit state and dynasm doesn't support rip relative addressing.
     *
     * eg on 64 bit:
     * 0-8: function ptr
     * 8-14: jmp aword [rip-14]
     * 
     * for 32 bit we only set the function ptr as it can always fit in a 32
     * bit displacement
     */
#if defined _WIN64 || defined __amd64__
    *(function_t*) code = func;
    code[8] = 0xFF; /* FF /4 operand for jmp */
    code[9] = 0x25; /* RIP displacement */
    *(int32_t*) &code[10] = -14;
    shred(code, 14, JUMP_SIZE);
#else
    *(function_t*) code = func;
    shred(code, 4, JUMP_SIZE);
#endif
}

void compile_globals(jit_t* jit, lua_State* L)
{
    jit_t* Dst = jit;
    dasm_setup(Dst, build_actionlist);

    /* Note: since the return code uses EBP to reset the stack pointer, we
     * don't have to track the amount of stack space used. It also means we
     * can handle stdcall and cdecl with the same code.
     */

    /* Note the various call_* functions want 32 bytes of 16 byte aligned
     * stack
     */

    |.if X64
    |.define L_ARG, r12
    |.define TOP, r13d
    |.define TOP_SAVE, r13
    |.else
    |.define L_ARG, rdi
    |.define TOP, esi
    |.define TOP_SAVE, rsi
    |.endif

    |.macro epilog
    |.if X64
    | mov TOP_SAVE, [rbp-16]
    | mov L_ARG, [rbp-8]
    |.else
    | mov TOP_SAVE, [rbp-8]
    | mov L_ARG, [rbp-4]
    |.endif
    | mov rsp, rbp
    | pop rbp
    | ret
    |.endmacro
    
    |.macro get_errno // note trashes registers
    | call extern GetLastError
    | call extern ERRNO
    | mov dword [rcx], eax
    |.endmacro

    /* the general idea for the return functions is:
     * 1) Save return value on stack
     * 2) Call get_errno (this trashes the registers hence #1)
     * 3) Unpack return value from stack
     * 4) Call lua push function
     * 5) Set eax to number of returned args (0 or 1)
     * 6) Call return which pops our stack frame
     */

    |->return:
    | mov eax, 1
    | epilog

    |->return_void:
    | mov ecx, dword [&jit->last_errno]
    | get_errno
    | mov eax, 0
    | epilog
    
    |->return_int:
    | mov [rsp+32], eax
    | get_errno
    |
    |.if X64WIN
    | cvtsi2sd xmm1, dword [rsp+32]
    | mov rcx, L_ARG
    |.elif X64
    | cvtsi2sd xmm0, dword [rsp+32]
    | mov rdi, L_ARG
    |.else
    | fild dword [rsp+32]
    | fstp qword [rsp+4]
    | mov [rsp], L_ARG
    |.endif
    |
    | call extern lua_pushnumber
    | jmp ->return

    |->return_double:
    |.if X64
    | movq qword [rsp+32], xmm0
    |.else
    | fstp qword [rsp+4] // note get_errno doesn't require any stack on x86
    |.endif
    |
    | get_errno
    |
    |.if X64WIN
    | movq xmm1, qword [rsp+32]
    | mov rcx, L_ARG
    |.elif X64
    | movq xmm0, qword [rsp+32]
    | mov rdi, L_ARG
    |.else
    | mov [rsp], L_ARG
    |.endif
    | call extern lua_pushnumber
    | jmp ->return

    |->return_bool:
    | mov [rsp+32], eax
    | get_errno
    | mov eax, [rsp+32]
    | call_pi, extern lua_pushboolean, L_ARG, eax
    | jmp ->return

    |->return_uint:
    | mov [rsp+32], eax
    | get_errno
    | mov eax, [rsp+32]
    | call_pi, extern push_uint, L_ARG, eax
    | jmp ->return

    |->too_few_arguments:
    | call_pp, extern luaL_error, L_ARG, &"too few arguments"

    |->too_many_arguments:
    | call_pp, extern luaL_error, L_ARG, &"too many arguments"

    compile(Dst, L, NULL);
}

int x86_stack_required(lua_State* L, int usr)
{
    size_t i;
    size_t argn = lua_rawlen(L, usr);
    int ret = 0;
    for (i = 1; i <= argn; i++) {
        const ctype_t* ct;
        lua_rawgeti(L, usr, (int) i);
        ct = (const ctype_t*) lua_touserdata(L, -1);

        if (ct->pointers) {
            ret += sizeof(void*);
        } else if (ct->type == DOUBLE_TYPE || ct->type == UINT64_TYPE || ct->type == INT64_TYPE) {
            ret += 8;
        } else if (ct->type == STRUCT_TYPE || ct->type == UNION_TYPE) {
            luaL_error(L, "NYI - structs as arguments");
        } else {
            /* other numeric types 4 bytes or less */
            ret += 4;
        }

        lua_pop(L, 1);
    }

    return ret;
}

#ifdef _WIN64
#define MAX_REGISTERS(ct) 4 /* rcx, rdx, r8, r9 */

#elif defined __amd64__
#define MAX_INT_REGISTERS(ct) 6 /* rdi, rsi, rdx, rcx, r8, r9 */
#define MAX_FLOAT_REGISTERS(ct) 8 /* xmm0-7 */

#else
#define MAX_INT_REGISTERS(ct) ((ct)->calling_convention == FAST_CALL ? 2 /* ecx, edx */ : 0)
#define MAX_FLOAT_REGISTERS(ct) 0
#endif

struct reg_alloc {
#ifdef _WIN64
    int regs;
    int is_float[4];
    int is_int[4];
#else
    int floats;
    int ints;
#endif
    size_t off;
};

#ifdef _WIN64
#define REGISTER_STACK_SPACE(ct) (4*8)
#elif defined __amd64__
#define REGISTER_STACK_SPACE(ct) (14*8)
#else
#define REGISTER_STACK_SPACE(ct) ((ct)->calling_convention == FAST_CALL ? 2*4 : 0)
#endif

/* Fastcall:
 * Uses ecx, edx as first two int registers
 * Everything else on stack (include 64bit ints)
 * No overflow stack space
 * Pops the stack before returning
 * Returns int in eax, float in ST0
 * We use the same register allocation logic as posix x64 with 2 int regs and 0 float regs
 */

void add_int32(Dst_DECL, const ctype_t* ct, struct reg_alloc* reg)
{
#ifdef _WIN64
    if (reg->regs < MAX_REGISTERS(ct)) {
        | mov [rsp + 32 + 8*(reg->regs)], rax
        reg->is_int[reg->regs++] = 1;
    } else
#else
    if (reg->ints < MAX_INT_REGISTERS(ct)) {
        | mov [rsp + 32 + sizeof(void*)*reg->ints], rax
        reg->ints++;
    } else
#endif

    {
        | mov [rsp+reg->off], eax
        reg->off += 4;
    }
}

void add_int64(Dst_DECL, const ctype_t* ct, struct reg_alloc* reg)
{
#if !defined _WIN64 && !defined __amd64__
    |.if not X64
    | mov [rsp + reg->off], RET_L
    | mov [rsp + reg->off + 4], RET_H
    |.endif
    reg->off += 8;
#else

#ifdef _WIN64
    if (reg->regs < MAX_REGISTERS(ct)) {
        | mov [rsp + 32 + 8*reg->regs], rax
        reg->is_int[reg->regs++] = 1;
#else
    if (reg->ints < MAX_INT_REGISTERS(ct)) {
        | mov [rsp + 32 + 8*reg->ints], rax
        reg->ints++;
#endif

    } else {
        | mov [rsp + reg->off], rax
        reg->off += 8;
    }
#endif
}

void add_double(Dst_DECL, const ctype_t* ct, struct reg_alloc* reg, int is_float)
{
#if !defined _WIN64 && !defined __amd64__
    |.if not X64
    assert(MAX_FLOAT_REGISTERS(ct) == 0);
    if (is_float) {
        | fstp dword [rsp + reg->off]
        reg->off += 4;
    } else {
        | fstp qword [rsp + reg->off]
        reg->off += 8;
    }
#else
    |.else

#ifdef _WIN64
    if (reg->regs < MAX_REGISTERS(ct)) {
        if (is_float) {
            | cvtpd2ps xmm0, xmm0
        }
        | movq qword [rsp + 32 + 8*(reg->regs)], xmm0
        reg->is_float[reg->regs++] = 1;
#else
    if (reg->floats < MAX_FLOAT_REGISTERS(ct)) {
        if (is_float) {
            | cvtpd2ps xmm0, xmm0
        }
        | movq qword [rsp + 32 + 8*(MAX_INT_REGISTERS(ct) + reg->floats)], xmm0
        reg->floats++;
#endif

    } else if (is_float) {
        | movd dword [rsp + reg->off], xmm0
        reg->off += 4;
    } else {
        | movq qword [rsp + reg->off], xmm0
        reg->off += 8;
    }
    |.endif
#endif
}

#if defined _WIN64 || defined __amd64__
#define add_pointer add_int64
#else
#define add_pointer add_int32
#endif

void push_function(jit_t* jit, lua_State* L, function_t func, int ct_usr, const ctype_t* ct)
{
    size_t i, nargs;
    int num_upvals;
    const ctype_t* mbr_ct;
    jit_t* Dst = jit;
    struct reg_alloc reg;

    memset(&reg, 0, sizeof(reg));
    reg.off = 32 + REGISTER_STACK_SPACE(ct);

    dasm_setup(Dst, build_actionlist);

    lua_pushvalue(L, ct_usr);
    ct_usr = lua_gettop(L);
    lua_pushvalue(L, CDATA_MT_UPVAL); /* so that CDATA_MT_UPVAL works within the closure */
    num_upvals = 2;

    nargs = lua_rawlen(L, ct_usr);

    if (ct->calling_convention != C_CALL && ct->has_var_arg) {
        luaL_error(L, "vararg is only allowed with the c calling convention");
    }

    | // int 3
    | push rbp
    | mov rbp, rsp
    | push L_ARG
    | push TOP_SAVE
    | // stack is 0 (mod 16) (TOP, L_ARG, rbp, rip)
    |
    |.if X64WIN
    | mov L_ARG, rcx
    | sub rsp, 32 // 32 bytes shadow space for lua_gettop
    | // leave rcx as is for call to lua_gettop
    |.elif X64
    | mov L_ARG, rdi
    | // leave rdi as is for call to lua_gettop
    |.else
    | mov L_ARG, [rbp + 8]
    | sub rsp, 16
    | mov [rsp], L_ARG
    |.endif
    |
    | call extern lua_gettop
    | mov TOP, eax
    | cmp eax, nargs
    | jl ->too_few_arguments

    if (!ct->has_var_arg) {
        | jg ->too_many_arguments
    }

    /* note movzxd rax, eax should be used here except it doesn't exist since
     * x86-64 guarentees that the upper 32 bits will always be zeroed when setting
     * eax */
    | shl rax, 4 // reserve 16 bytes per argument - this maintains the alignment mod 16
    | sub rsp, rax
    | sub rsp, 32 + REGISTER_STACK_SPACE(ct) // reserve an extra 32 to call local functions

    for (i = 1; i <= nargs; i++) {
        lua_rawgeti(L, ct_usr, (int) i);
        mbr_ct = (const ctype_t*) lua_touserdata(L, -1);
        
        if (mbr_ct->pointers) {
            lua_getuservalue(L, -1);
            num_upvals += 2;
            | call_piip, extern to_typed_pointer, L_ARG, i, lua_upvalueindex(num_upvals), mbr_ct
            add_pointer(Dst, ct, &reg);
        } else {
            switch (mbr_ct->type) {
            case FUNCTION_TYPE:
                lua_getuservalue(L, -1);
                num_upvals += 2;
                | call_piip, extern to_typed_pointer, L_ARG, i, lua_upvalueindex(num_upvals), mbr_ct
                add_pointer(Dst, ct, &reg);
                break;

            case ENUM_TYPE:
                lua_getuservalue(L, -1);
                num_upvals += 2;
                | call_piip, extern to_enum, L_ARG, i, lua_upvalueindex(num_upvals), mbr_ct
                add_int32(Dst, ct, &reg);
                break;

            case INT8_TYPE:
                | call_pi, extern to_int32, L_ARG, i
                | movsx eax, al
                add_int32(Dst, ct, &reg);
                lua_pop(L, 1);
                break;

            case UINT8_TYPE:
                | call_pi, extern to_uint32, L_ARG, i
                | movzx eax, al
                add_int32(Dst, ct, &reg);
                lua_pop(L, 1);
                break;

            case INT16_TYPE:
                | call_pi, extern to_int32, L_ARG, i
                | movsx eax, ax
                add_int32(Dst, ct, &reg);
                lua_pop(L, 1);
                break;

            case UINT16_TYPE:
                | call_pi, extern to_uint32, L_ARG, i
                | movzx eax, ax
                add_int32(Dst, ct, &reg);
                lua_pop(L, 1);
                break;

            case INT32_TYPE:
                | call_pi, extern to_int32, L_ARG, i
                add_int32(Dst, ct, &reg);
                lua_pop(L, 1);
                break;

            case UINT32_TYPE:
                | call_pi, extern to_uint32, L_ARG, i
                add_int32(Dst, ct, &reg);
                lua_pop(L, 1);
                break;

            case UINTPTR_TYPE:
                | call_pi, extern to_uintptr, L_ARG, i
                add_pointer(Dst, ct, &reg);
                lua_pop(L, 1);
                break;
            
            case INT64_TYPE:
                | call_pi, extern to_int64, L_ARG, i
                add_int64(Dst, ct, &reg);
                lua_pop(L, 1);
                break;
            
            case UINT64_TYPE:
                | call_pi, extern to_uint64, L_ARG, i
                add_int64(Dst, ct, &reg);
                lua_pop(L, 1);
                break;

            case DOUBLE_TYPE:
                | call_pi, extern to_double, L_ARG, i
                add_double(Dst, ct, &reg, 0);
                lua_pop(L, 1);
                break;

            case FLOAT_TYPE:
                | call_pi, extern to_double, L_ARG, i
                add_double(Dst, ct, &reg, 1);
                lua_pop(L, 1);
                break;

            default:
                luaL_error(L, "NYI: call arg type");
            }
        }
    }

    if (ct->has_var_arg) {
#ifdef _WIN64
        |.if X64WIN
        if (reg.regs < MAX_REGISTERS(ct)) {
            assert(reg.regs == nargs);
            | cmp TOP, MAX_REGISTERS(ct)
            | jle >1
            | // unpack onto stack
            | mov rax, rsp
            | add rax, 32 + 8*MAX_REGISTERS(ct)
            | call_piip, extern unpack_varargs_stack, L_ARG, MAX_REGISTERS(ct)+1, TOP, rax
            | // unpack to registers
            | mov rax, rsp
            | add rax, 32 + 8*(reg.regs)
            | call_piip, extern unpack_varargs_reg, L_ARG, nargs+1, MAX_REGISTERS(ct), rax
            | jmp >2
            |1:
            | // unpack just to registers
            | mov rax, rsp
            | add rax, 32 + 8*(reg.regs)
            | call_piip, extern unpack_varargs_reg, L_ARG, nargs+1, TOP, rax
            |2:
        } else {
            | // unpack just to stack
            | mov rax, rsp
            | add rax, reg.off
            | call_piip, extern unpack_varargs_stack, L_ARG, nargs+1, TOP, rax
        }

        for (i = nargs; i < MAX_REGISTERS(ct); i++) {
            reg.is_int[i] = reg.is_float[i] = 1;
        }
        reg.regs = MAX_REGISTERS(ct);
#elif defined __amd64__
        |.elif X64
        if (reg.floats < MAX_FLOAT_REGISTERS(ct)) {
            | mov rax, rsp
            | add rax, 32 + 8*(MAX_INT_REGISTERS(ct) + reg.floats)
            | call_piiip, extern unpack_varargs_float, L_ARG, nargs+1, TOP, MAX_FLOAT_REGISTERS(ct) - reg.floats, rax
        }

        if (reg.ints < MAX_INT_REGISTERS(ct)) {
            | mov rax, rsp
            | add rax, 32 + 8*(reg.ints)
            | call_piiip, extern unpack_varargs_int, L_ARG, nargs+1, TOP, MAX_INT_REGISTERS(ct) - reg.ints, rax
        }

        | mov rax, rsp
        | add rax, reg.off
        | call_piiiip, extern unpack_varargs_stack_skip, L_ARG, nargs+1, TOP, MAX_INT_REGISTERS(ct) - reg.ints, MAX_FLOAT_REGISTERS(ct) - reg.floats, rax

        reg.floats = MAX_FLOAT_REGISTERS(ct);
        reg.ints = MAX_INT_REGISTERS(ct);
#else
        |.else
        | mov rax, rsp
        | add rax, reg.off
        | call_piip, extern unpack_varargs_stack, L_ARG, nargs+1, TOP, rax
        |.endif
#endif
    }

    | call extern ERRNO
    | mov eax, dword [rcx]
    | call_i, extern SetLastError, eax

    /* remove the stack space to call local functions */
    |.if X32WIN
    | add rsp, 28 // SetLastError will have already popped 4
    |.else
    | add rsp, 32
    |.endif

#ifdef _WIN64
    |.if X64WIN
    switch (reg.regs) {
    case 4:
        if (reg.is_float[3]) {
            | movq xmm3, qword [rsp + 8*3]
        }
        if (reg.is_int[3]) {
            | mov r9, [rsp + 8*3]
        }
    case 3:
        if (reg.is_float[2]) {
            | movq xmm2, qword [rsp + 8*2]
        }
        if (reg.is_int[2]) {
            | mov r8, [rsp + 8*2]
        }
    case 2:
        if (reg.is_float[1]) {
            | movq xmm1, qword [rsp + 8*1]
        }
        if (reg.is_int[1]) {
            | mov rdx, [rsp + 8*1]
        }
    case 1:
        if (reg.is_float[0]) {
            | movq xmm0, qword [rsp]
        }
        if (reg.is_int[0]) {
            | mov rcx, [rsp]
        }
    case 0:
        break;
    }

    /* don't remove the space for the registers as we need 32 bytes of register overflow space */
    assert(REGISTER_STACK_SPACE(ct) == 32);

#elif defined __amd64__
    |.elif X64
    switch (reg.floats) {
    case 8:
        | movq xmm7, qword [rsp + 8*(MAX_INT_REGISTERS(ct)+7)]
    case 7: 
        | movq xmm6, qword [rsp + 8*(MAX_INT_REGISTERS(ct)+6)]
    case 6: 
        | movq xmm5, qword [rsp + 8*(MAX_INT_REGISTERS(ct)+5)]
    case 5: 
        | movq xmm4, qword [rsp + 8*(MAX_INT_REGISTERS(ct)+4)]
    case 4:
        | movq xmm3, qword [rsp + 8*(MAX_INT_REGISTERS(ct)+3)]
    case 3:
        | movq xmm2, qword [rsp + 8*(MAX_INT_REGISTERS(ct)+2)]
    case 2:
        | movq xmm1, qword [rsp + 8*(MAX_INT_REGISTERS(ct)+1)]
    case 1:
        | movq xmm0, qword [rsp + 8*(MAX_INT_REGISTERS(ct))]
    case 0:
        break;
    }

    switch (reg.ints) {
    case 6:
        | mov r9, [rsp + 8*5]
    case 5:
        | mov r8, [rsp + 8*4]
    case 4:
        | mov rcx, [rsp + 8*3]
    case 3:
        | mov rdx, [rsp + 8*2]
    case 2:
        | mov rsi, [rsp + 8*1]
    case 1:
        | mov rdi, [rsp]
    case 0:
        break;
    }

    | add rsp, REGISTER_STACK_SPACE(ct)
#else
    |.else
    if (ct->calling_convention == FAST_CALL) {
        switch (reg.ints) {
        case 2:
            | mov edx, [rsp + 4]
        case 1:
            | mov ecx, [rsp]
        case 0:
            break;
        }

        | add rsp, REGISTER_STACK_SPACE(ct)
    }
    |.endif
#endif

#ifdef __amd64__
    if (ct->has_var_arg) {
        /* al stores an upper limit on the number of float register, note that
         * its allowed to be more than the actual number of float registers used as
         * long as its 0-8 */
        |.if X64 and not X64WIN
        | mov al, 8
        |.endif
    }
#endif

    | call extern FUNCTION
    | sub rsp, 48 // 32 to be able to call local functions, 16 so we can store some local variables

    /* note on windows X86 the stack may be only aligned to 4 (stdcall will
     * have popped a multiple of 4 bytes), but we don't need 16 byte alignment on
     * that platform
     */

    lua_rawgeti(L, ct_usr, 0);
    mbr_ct = (const ctype_t*) lua_touserdata(L, -1);

    if (mbr_ct->pointers || mbr_ct->type == UINTPTR_TYPE) {
        lua_getuservalue(L, -1);
        num_upvals += 2;
        | mov [rsp+32], rax // save the pointer
        | get_errno
        | call_pip, extern push_cdata, L_ARG, lua_upvalueindex(num_upvals), mbr_ct
        | mov rcx, [rsp+32]
        | mov [rax], rcx // *(void**) cdata = val
        | jmp ->return

    } else {
        switch (mbr_ct->type) {
        case INT64_TYPE:
        case UINT64_TYPE:
            num_upvals++;
            | // save the return value
            |.if X64
            | mov [rsp+32], rax
            |.else
            | mov [rsp+36], edx // high
            | mov [rsp+32], eax // low
            |.endif
            |
            | get_errno
            | call_pip, extern push_cdata, L_ARG, lua_upvalueindex(num_upvals), mbr_ct
            |
            | // *(int64_t*) cdata = val
            |.if X64
            | mov rcx, [rsp+32]
            | mov [rax], rcx
            |.else
            | mov rcx, [rsp+36]
            | mov rdx, [rsp+32]
            | mov [rax+4], rcx
            | mov [rax], rdx
            |.endif
            |
            | jmp ->return
            break;

        case VOID_TYPE:
            | jmp ->return_void
            lua_pop(L, 1);
            break;

        case BOOL_TYPE:
            | jmp ->return_bool
            lua_pop(L, 1);
            break;

        case INT8_TYPE:
            | movsx eax, al
            | jmp ->return_int
            lua_pop(L, 1);
            break;

        case INT16_TYPE:
            | movsx eax, ax
            | jmp ->return_int
            lua_pop(L, 1);
            break;

        case UINT8_TYPE:
            | movzx eax, al
            | jmp ->return_int
            lua_pop(L, 1);
            break;

        case UINT16_TYPE:
            | movzx eax, ax
            | jmp ->return_int
            lua_pop(L, 1);
            break;

        case INT32_TYPE:
        case ENUM_TYPE:
            | jmp ->return_int
            lua_pop(L, 1);
            break;

        case UINT32_TYPE:
            | jmp ->return_uint
            lua_pop(L, 1);
            break;

        case FLOAT_TYPE:
            |.if X64
            | cvtps2pd xmm0, xmm0
            |.endif
        case DOUBLE_TYPE:
            | jmp ->return_double
            lua_pop(L, 1);
            break;

        default:
            luaL_error(L, "NYI: call return type");
        }
    }

    assert(lua_gettop(L) == ct_usr + num_upvals - 1);
    lua_pushcclosure(L, (lua_CFunction) compile(Dst, L, func), num_upvals);
}

